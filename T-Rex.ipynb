{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import hypot\n",
    "import pyautogui\n",
    "import dlib\n",
    "from dlib import shape_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights =  \"C:/Users/aashi/Downloads/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "# Path to the architecture file\n",
    "model_arch = \"C:/Users/aashi/Downloads/deploy.prototxt.txt\"\n",
    "# Load the caffe model\n",
    "net = cv2.dnn.readNetFromCaffe(model_arch, model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image, threshold =0.7):\n",
    "    \n",
    "    # Get the height,width of the image\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Apply mean subtraction, and create 4D blob from image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0,(300, 300), (104.0, 117.0, 123.0))\n",
    "    \n",
    "    # Set the new input value for the network\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Run forward pass on the input to compute output\n",
    "    faces = net.forward()\n",
    "    \n",
    "    # Get the confidence value for all detected faces\n",
    "    prediction_scores = faces[:,:,:,2]\n",
    "    \n",
    "    # Get the index of the prediction with highest confidence \n",
    "    i = np.argmax(prediction_scores)\n",
    "    \n",
    "    # Get the face with highest confidence \n",
    "    face = faces[0,0,i]\n",
    "    \n",
    "    # Extract the confidence\n",
    "    confidence = face[2]\n",
    "    \n",
    "    # if confidence value is greater than the threshold\n",
    "    if confidence > threshold:\n",
    "        \n",
    "        # The 4 values at indexes 3 to 6 are the top-left, bottom-right coordinates\n",
    "        # scales to range 0-1.The original coordinates can be found by \n",
    "        # multiplying x,y values with the width,height of the image\n",
    "        box = face[3:7] * np.array([w, h, w, h])\n",
    "        \n",
    "        # The coordinates are the pixel numbers relative to the top left\n",
    "        # corner of the image therefore needs be quantized to int type\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "        \n",
    "        # Draw a bounding box around the face.\n",
    "        annotated_frame = cv2.rectangle(image.copy(), (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        output = (annotated_frame, (x1, y1, x2, y2), True, confidence)\n",
    "    \n",
    "    # Return the original frame if no face is detected with high confidence.\n",
    "    else:\n",
    "        output = (image,(),False, 0)\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "\n",
    "cv2.namedWindow('face Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "\n",
    "    # Detect face in the frame\n",
    "    annotated_frame, coords, status, conf = face_detector(frame)\n",
    "    # Display the frame\n",
    "    cv2.imshow('face Detection', annotated_frame)\n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor(\"C:/Users/aashi/Downloads/shape_predictor_68_face_landmarks.dat\")\n",
    "def detect_landmarks(box, image):\n",
    "    \n",
    "    # For faster results convert the image to gray-scale\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get the coordinates \n",
    "    (x1, y1, x2, y2) = box\n",
    "\n",
    "    # Perform the detection\n",
    "    shape = predictor(gray_scale, dlib.rectangle(x1, y1, x2, y2))\n",
    "    \n",
    "    # Get the numPy array containing the coordinates of the landmarks\n",
    "    landmarks = shape_to_np(shape)\n",
    "    \n",
    "   # Draw the landmark points with circles \n",
    "    for (x, y) in landmarks:\n",
    "        annotated_image = cv2.circle(image, (x, y),2, (0, 127, 255), -1)\n",
    "\n",
    "    return annotated_image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape):\n",
    "\n",
    "    # Create an array of shape (68, 2) for storing the landmark coordinates\n",
    "    landmarks = np.zeros((68, 2), dtype=\"int\")\n",
    "\n",
    "    # Write the x,y coordinates of each landmark into the array\n",
    "\n",
    "    for i in range(0, 68):\n",
    "\n",
    "        landmarks[i] = (shape.part(i).x, shape.part(i).y)\n",
    "         \n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Landmark Detection', cv2.WINDOW_NORMAL)\n",
    "while(True):\n",
    "\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break if frame is not returned\n",
    "\n",
    "    if not ret:\n",
    "\n",
    "        break\n",
    "\n",
    "         \n",
    "\n",
    "    # Flip the frame\n",
    "\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "\n",
    "     \n",
    "\n",
    "    # Detect face in the fram\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    if status:\n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame) \n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Landmark Detection',landmark_image)     \n",
    "\n",
    "    # Break the loop if 'q' key pressed\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\n",
    "        break\n",
    "\n",
    " \n",
    "\n",
    "# When everything is done, release the capture and destroy the window\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(landmarks, ar_threshold = 0.7): \n",
    "    \n",
    "    \n",
    "    # Calculate the euclidean distance labelled as A,B,C\n",
    "    A = hypot(landmarks[50][0] - landmarks[58][0], landmarks[50][1] - landmarks[58][1])\n",
    "    B = hypot(landmarks[52][0] - landmarks[56][0], landmarks[52][1] - landmarks[56][1])\n",
    "    C = hypot(landmarks[48][0] - landmarks[54][0], landmarks[48][1] - landmarks[54][1])\n",
    "    \n",
    "    # Calculate the mouth aspect ratio\n",
    "    # The value of vertical distance A,B is averaged\n",
    "    mouth_aspect_ratio = (A + B) / (2.0 * C)\n",
    "    \n",
    "    # Return True if the value is greater than the threshold\n",
    "    if mouth_aspect_ratio > ar_threshold:\n",
    "        return True, mouth_aspect_ratio\n",
    "    else:\n",
    "        return False, mouth_aspect_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Mouth Status', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Adjust the threshold and make sure it's working for you.\n",
    "        mouth_status,_ = is_mouth_open(landmarks, ar_threshold = 0.6)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(frame,'Is Mouth Open: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Mouth Status',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_proximity(box,image, proximity_threshold = 325):\n",
    "    \n",
    "    # Get the height and width of the face bounding box\n",
    "    face_width =  box[2]-box[0]\n",
    "    face_height = box[3]-box[1]\n",
    "    \n",
    "    # Draw rectangle to guide the user \n",
    "    # Calculate the angle of diagonal using face width, height \n",
    "    theta = np.arctan(face_height/face_width)\n",
    "     \n",
    "    # Use the angle to calculate height, width of the guide rectangle\n",
    "    guide_height = np.sin(theta)*proximity_threshold\n",
    "    guide_width  = np.cos(theta)*proximity_threshold\n",
    "    \n",
    "    # Calculate the mid-point of the guide rectangle/face bounding box\n",
    "    mid_x,mid_y = (box[2]+box[0])/2 , (box[3]+box[1])/2\n",
    "    \n",
    "    # Calculate the coordinates of top-left and bottom-right corners\n",
    "    guide_topleft = int(mid_x-(guide_width/2)), int(mid_y-(guide_height/2))\n",
    "    guide_bottomright = int(mid_x +(guide_width/2)), int(mid_y + (guide_height/2))\n",
    "    \n",
    "    # Draw the guide rectangle\n",
    "    cv2.rectangle(image, guide_topleft, guide_bottomright, (0, 255, 255), 2)\n",
    "    \n",
    "    # Calculate the diagonal distance of the bounding box\n",
    "    diagonal = hypot(face_width, face_height)\n",
    "    \n",
    "    # Return True if distance greater than the threshold\n",
    "    if diagonal > proximity_threshold:\n",
    "        return True, diagonal\n",
    "    else:\n",
    "        return False, diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Face proximity', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Check if face is closer than the defined threshold\n",
    "        is_face_close,_ = face_proximity(box_coords, face_image, proximity_threshold = 325)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(face_image,'Is Face Close: {}'.format(is_face_close),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "        \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face proximity',face_image)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Calibration', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Detect landmarks if the frame is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Get the current mouth aspect ratio\n",
    "        _,mouth_ar = is_mouth_open(landmarks)\n",
    "    \n",
    "        # Get the current face proximity\n",
    "        _, proximity  = face_proximity(box_coords, face_image)\n",
    "\n",
    "        # Calculate threshold values\n",
    "        ar_threshold = mouth_ar*1.4\n",
    "        proximity_threshold = proximity*1.3\n",
    "\n",
    "        \n",
    "        # Dsiplay the threshold values\n",
    "        cv2.putText(frame, 'Aspect ratio threshold: {:.2f} '.format(ar_threshold), \n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        cv2.putText(frame,'Proximity threshold: {:.2f}'.format(proximity_threshold), \n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "     \n",
    "    # Display the frame\n",
    "    cv2.imshow('Calibration',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ran\n"
     ]
    }
   ],
   "source": [
    "pyautogui.click(button='right')\n",
    "pyautogui.press('space')\n",
    "pyautogui.press(['shift','enter'])\n",
    "pyautogui.keyDown('shift')\n",
    "pyautogui.press('enter')\n",
    "\n",
    "# Release the shift key\n",
    "pyautogui.keyUp('shift')\n",
    "print('I ran')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailSafeException",
     "evalue": "PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailSafeException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10252/1673684478.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# Else the space key is Up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mpyautogui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyUp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'space'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mmouth_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Closed'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Aashish Folder\\Apps\\Anaconda Navigator\\envs\\env_New\\lib\\site-packages\\pyautogui\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrappedFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[0mfailSafeCheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[0mreturnVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrappedFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0m_handlePause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_pause\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Aashish Folder\\Apps\\Anaconda Navigator\\envs\\env_New\\lib\\site-packages\\pyautogui\\__init__.py\u001b[0m in \u001b[0;36mfailSafeCheck\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mFAILSAFE\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFAILSAFE_POINTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m         raise FailSafeException(\n\u001b[1;32m-> 1723\u001b[1;33m             \u001b[1;34m\"PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1724\u001b[0m         )\n\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailSafeException\u001b[0m: PyAutoGUI fail-safe triggered from mouse moving to a corner of the screen. To disable this fail-safe, set pyautogui.FAILSAFE to False. DISABLING FAIL-SAFE IS NOT RECOMMENDED."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Dino with OpenCV', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# By default each key press is followed by a 0.1 second pause\n",
    "pyautogui.PAUSE = 0.0\n",
    "\n",
    "# The fail-safe triggers an exception in case mouse\n",
    "# is moved to corner of the screen\n",
    "#pyautogui.FAILSAFE = False\n",
    "\n",
    "while(True):\n",
    "    \n",
    "     # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Detect landmarks if a face is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Check if mouth is open\n",
    "        is_open,_ = is_mouth_open(landmarks, ar_threshold)\n",
    "        \n",
    "        # If the mouth is open trigger space key Down event to jump\n",
    "        if is_open:\n",
    "            \n",
    "            pyautogui.keyDown('space')\n",
    "            mouth_status = 'Open'\n",
    "\n",
    "        else:\n",
    "            # Else the space key is Up\n",
    "            pyautogui.keyUp('space')\n",
    "            mouth_status = 'Closed'\n",
    "        \n",
    "        # Display the mouth status on the frame\n",
    "        cv2.putText(frame,'Mouth: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        # Check the proximity of the face\n",
    "        is_closer,_  = face_proximity(box_coords, frame, proximity_threshold)\n",
    "        \n",
    "        # If face is closer press the down key            \n",
    "        if is_closer:\n",
    "            pyautogui.keyDown('down')\n",
    "            \n",
    "        else:\n",
    "            pyautogui.keyUp('down')\n",
    "        \n",
    "    # Display the frame\n",
    "    cv2.imshow('Dino with OpenCV',frame)\n",
    "\n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
